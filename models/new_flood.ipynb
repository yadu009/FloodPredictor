{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951a2fb5-2705-4122-8510-da66ed90d330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 GENERALIZED PHYSICAL FACTORS FLOOD FORECASTING SYSTEM\n",
      "============================================================\n",
      "🔄 Generating 10000 generalized samples...\n",
      "✅ Generalized dataset created! Shape: (10000, 14), Floods: 966\n",
      "🔧 Training model on generalized physical factors...\n",
      "✅ Model trained successfully! Test Accuracy: 0.957\n",
      "\n",
      "📊 Detailed Performance Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.96      0.99      0.98      2710\n",
      "       Flood       0.88      0.65      0.75       290\n",
      "\n",
      "    accuracy                           0.96      3000\n",
      "   macro avg       0.92      0.82      0.86      3000\n",
      "weighted avg       0.96      0.96      0.95      3000\n",
      "\n",
      "\n",
      "============================================================\n",
      "🎉 SYSTEM READY FOR GENERALIZED REAL-TIME PREDICTIONS!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🌊 Generalized Physical Factors Flood Forecasting System - Jupyter Implementation\n",
    "# Real-time predictions for any location based on physical data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🌊 GENERALIZED PHYSICAL FACTORS FLOOD FORECASTING SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class PhysicalFloodPredictor:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        # Feature columns no longer include 'district_encoded'\n",
    "        self.feature_columns = [\n",
    "            'rainfall_mm', 'river_discharge_cumec', 'water_level_m',\n",
    "            'soil_moisture_percent', 'temperature_c', 'humidity_percent', 'wind_speed_ms',\n",
    "            'pressure_hpa', 'elevation_m', 'population_density', 'drainage_efficiency',\n",
    "            'distance_to_coast_km', 'deforestation_index'\n",
    "        ]\n",
    "\n",
    "    def generate_physical_dataset(self, n_samples=10000):\n",
    "        \"\"\"Generate a generalized dataset with randomized physical factors.\"\"\"\n",
    "        print(f\"🔄 Generating {n_samples} generalized samples...\")\n",
    "        np.random.seed(42)\n",
    "        data = []\n",
    "\n",
    "        # Define plausible ranges for generalized locations in a country like India\n",
    "        MAX_ELEVATION = 2500 # Capping at a reasonable max for inhabited areas\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            # Generate location-agnostic physical properties\n",
    "            elevation = np.random.uniform(0, MAX_ELEVATION)\n",
    "            population_density = max(10, np.random.lognormal(mean=6, sigma=1.5)) # Skewed towards more common densities\n",
    "            distance_to_coast = np.random.uniform(0, 1500)\n",
    "            drainage_efficiency = np.random.uniform(0.1, 0.9)\n",
    "            deforestation = np.random.uniform(0.05, 0.9)\n",
    "\n",
    "            # --- The rest of the generation logic remains similar ---\n",
    "            rainfall_scenario = np.random.random()\n",
    "            if rainfall_scenario > 0.95:\n",
    "                rainfall = np.random.uniform(150, 300)\n",
    "            elif rainfall_scenario > 0.85:\n",
    "                rainfall = np.random.uniform(50, 150)\n",
    "            else:\n",
    "                rainfall = max(0, np.random.exponential(15))\n",
    "\n",
    "            river_discharge = max(0, (rainfall * 0.5 * (1 + elevation / 1000)) + np.random.normal(0, 30))\n",
    "            water_level = max(0, 1.5 + (river_discharge / 300) + np.random.normal(0, 0.8))\n",
    "            soil_moisture = min(100, max(15, 35 + rainfall * 1.5 + np.random.normal(0, 12)))\n",
    "            temperature = np.random.normal(25, 6)\n",
    "            humidity = min(100, max(40, 60 + rainfall * 0.7))\n",
    "            wind_speed = max(0, np.random.normal(10, 8))\n",
    "            pressure = np.random.normal(1010, 12) - (rainfall / 10) # Lower pressure with more rain\n",
    "\n",
    "            # Calculate comprehensive flood risk score\n",
    "            flood_risk_score = (\n",
    "                (rainfall / 250) * 0.30 +\n",
    "                (river_discharge / 800) * 0.25 +\n",
    "                (water_level / 8) * 0.20 +\n",
    "                (soil_moisture / 100) * 0.08 +\n",
    "                ((MAX_ELEVATION - elevation) / MAX_ELEVATION) * 0.07 + # Low elevation risk\n",
    "                (deforestation) * 0.05 +\n",
    "                ((1 - drainage_efficiency)) * 0.03 +\n",
    "                (min(population_density, 5000) / 5000) * 0.02 # Capped population pressure\n",
    "            )\n",
    "\n",
    "            flood_occurred = 1 if flood_risk_score > 0.40 else 0\n",
    "            if np.random.random() < 0.03: # Add noise\n",
    "                flood_occurred = 1 - flood_occurred\n",
    "\n",
    "            sample = [\n",
    "                rainfall, river_discharge, water_level, soil_moisture,\n",
    "                temperature, humidity, wind_speed, pressure, elevation,\n",
    "                population_density, drainage_efficiency, distance_to_coast,\n",
    "                deforestation, flood_occurred\n",
    "            ]\n",
    "            data.append(sample)\n",
    "\n",
    "        columns = self.feature_columns + ['flood_occurred']\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        print(f\"✅ Generalized dataset created! Shape: {df.shape}, Floods: {df['flood_occurred'].sum()}\")\n",
    "        return df\n",
    "\n",
    "    def train_model(self, df=None):\n",
    "        \"\"\"Train flood prediction model on the generalized dataset.\"\"\"\n",
    "        if df is None:\n",
    "            df = self.generate_physical_dataset()\n",
    "\n",
    "        print(\"🔧 Training model on generalized physical factors...\")\n",
    "        X = df[self.feature_columns]\n",
    "        y = df['flood_occurred']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "        self.model = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=8, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        test_pred = self.model.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        print(f\"✅ Model trained successfully! Test Accuracy: {test_acc:.3f}\")\n",
    "        print(\"\\n📊 Detailed Performance Report:\")\n",
    "        print(classification_report(y_test, test_pred, target_names=['Normal', 'Flood']))\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict_flood_risk(self, **kwargs):\n",
    "        \"\"\"Predict flood risk from any location's physical conditions.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"❌ Model not trained! Please run train_model() first.\")\n",
    "\n",
    "        # Validate that all required features are present\n",
    "        for param in self.feature_columns:\n",
    "            if param not in kwargs:\n",
    "                raise ValueError(f\"❌ Missing parameter: {param}\")\n",
    "\n",
    "        # Prepare feature vector in the exact order used for training\n",
    "        features = np.array([kwargs[param] for param in self.feature_columns]).reshape(1, -1)\n",
    "\n",
    "        prediction = self.model.predict(features)[0]\n",
    "        probability = self.model.predict_proba(features)[0][1]\n",
    "\n",
    "        # Risk assessment and recommendation logic\n",
    "        if probability >= 0.85:\n",
    "            risk_level = \"🚨 CRITICAL\"\n",
    "            recommendation = \"IMMEDIATE EVACUATION may be required! Contact authorities.\"\n",
    "        elif probability >= 0.65:\n",
    "            risk_level = \"🔴 HIGH\"\n",
    "            recommendation = \"Take immediate precautionary measures. Prepare for potential evacuation.\"\n",
    "        elif probability >= 0.45:\n",
    "            risk_level = \"🟠 MEDIUM\"\n",
    "            recommendation = \"Monitor conditions closely. Prepare emergency supplies.\"\n",
    "        elif probability >= 0.25:\n",
    "            risk_level = \"🟡 LOW\"\n",
    "            recommendation = \"Stay alert and monitor weather updates.\"\n",
    "        else:\n",
    "            risk_level = \"🟢 SAFE\"\n",
    "            recommendation = \"Current conditions appear normal.\"\n",
    "\n",
    "        return {\n",
    "            'prediction': 'FLOOD WARNING' if prediction == 1 else 'NORMAL CONDITIONS',\n",
    "            'probability': round(probability, 3),\n",
    "            'risk_level': risk_level,\n",
    "            'recommendation': recommendation,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Initialize and train the predictor\n",
    "predictor = PhysicalFloodPredictor()\n",
    "predictor.train_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 SYSTEM READY FOR GENERALIZED REAL-TIME PREDICTIONS!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203c326d-690e-4bce-82cb-87807385f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 Flood Risk Assessment Results\n",
      "-----------------------------------\n",
      "Prediction:     NORMAL CONDITIONS\n",
      "Probability:    15.7%\n",
      "Risk Level:     🟢 SAFE\n",
      "Recommendation: Current conditions appear normal.\n",
      "Timestamp:      2025-10-07 13:03:39\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Sample Prediction: High-Risk Coastal Scenario ---\n",
    "\n",
    "# Define the physical conditions of the location you want to check\n",
    "high_risk_conditions = {\n",
    "    'rainfall_mm': 100.0,              # Very heavy rainfall\n",
    "    'river_discharge_cumec': 250.0,    # High river flow\n",
    "    'water_level_m': 4.5,              # Dangerously high water level\n",
    "    'soil_moisture_percent': 95.0,     # Soil is completely saturated\n",
    "    'temperature_c': 24.0,             # Cooler storm temperature\n",
    "    'humidity_percent': 98.0,          # Extremely high humidity\n",
    "    'wind_speed_ms': 20.0,             # Strong winds\n",
    "    'pressure_hpa': 998.0,             # Low atmospheric pressure (storm)\n",
    "    'elevation_m': 10.0,               # Very low-lying area\n",
    "    'population_density': 1800.0,      # Densely populated urban area\n",
    "    'drainage_efficiency': 0.4,        # Poor drainage system\n",
    "    'distance_to_coast_km': 5.0,       # Very close to the coast\n",
    "    'deforestation_index': 0.7         # High deforestation, causing more runoff\n",
    "}\n",
    "\n",
    "# Get the prediction by unpacking the dictionary as keyword arguments\n",
    "try:\n",
    "    result = predictor.predict_flood_risk(**high_risk_conditions)\n",
    "\n",
    "    # Print the results in a user-friendly format\n",
    "    print(f\"🌊 Flood Risk Assessment Results\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"Prediction:     {result['prediction']}\")\n",
    "    print(f\"Probability:    {result['probability'] * 100:.1f}%\")\n",
    "    print(f\"Risk Level:     {result['risk_level']}\")\n",
    "    print(f\"Recommendation: {result['recommendation']}\")\n",
    "    print(f\"Timestamp:      {result['timestamp']}\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1de3b92-3dc9-4109-bb21-0f4dee22de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 GENERALIZED PHYSICAL FACTORS FLOOD FORECASTING SYSTEM\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 🌊 GENERALIZED PHYSICAL FACTORS FLOOD FORECASTING SYSTEM\n",
    "# =============================================\n",
    "# Real-time prediction version — ready for input/output demonstration\n",
    "# Author: Team DTU TECH TRAILBLAZERS (Optimized for competition use)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🌊 GENERALIZED PHYSICAL FACTORS FLOOD FORECASTING SYSTEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLASS DEFINITION\n",
    "# ---------------------------------------------\n",
    "class PhysicalFloodPredictor:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        self.feature_columns = [\n",
    "            'rainfall_mm', 'river_discharge_cumec', 'water_level_m',\n",
    "            'soil_moisture_percent', 'temperature_c', 'humidity_percent', 'wind_speed_ms',\n",
    "            'pressure_hpa', 'elevation_m', 'population_density', 'drainage_efficiency',\n",
    "            'distance_to_coast_km', 'deforestation_index'\n",
    "        ]\n",
    "\n",
    "    def generate_physical_dataset(self, n_samples=8000):\n",
    "        np.random.seed(42)\n",
    "        MAX_ELEVATION = 3000\n",
    "        data = []\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            elevation = np.random.exponential(scale=300)\n",
    "            elevation = min(elevation, MAX_ELEVATION)\n",
    "            rainfall = np.random.exponential(20)\n",
    "            if np.random.random() < 0.1:\n",
    "                rainfall += np.random.uniform(100, 300)\n",
    "\n",
    "            river_discharge = max(0, rainfall * 0.6 + np.random.normal(0, 20))\n",
    "            water_level = max(0, 1.5 + river_discharge / 300 + np.random.normal(0, 0.4))\n",
    "            soil_moisture = np.clip(25 + rainfall * 1.5 + np.random.normal(0, 10), 5, 100)\n",
    "            temperature = np.random.normal(25 - (elevation / 800), 5)\n",
    "            humidity = np.clip(50 + rainfall * 0.5 + np.random.normal(0, 10), 10, 100)\n",
    "            wind_speed = abs(np.random.normal(8, 5))\n",
    "            pressure = np.random.normal(1012 - rainfall / 10, 8)\n",
    "            population_density = np.random.lognormal(mean=6, sigma=1.2)\n",
    "            drainage_efficiency = np.clip(np.random.beta(2, 5), 0.1, 0.95)\n",
    "            distance_to_coast = np.random.uniform(0, 1500)\n",
    "            deforestation = np.random.uniform(0.05, 0.9)\n",
    "\n",
    "            flood_score = (\n",
    "                (rainfall / 300) * 0.35 +\n",
    "                (river_discharge / 800) * 0.25 +\n",
    "                (water_level / 10) * 0.15 +\n",
    "                (soil_moisture / 100) * 0.08 +\n",
    "                (1 - drainage_efficiency) * 0.05 +\n",
    "                (deforestation) * 0.05 +\n",
    "                ((MAX_ELEVATION - elevation) / MAX_ELEVATION) * 0.05 +\n",
    "                (min(population_density, 5000) / 5000) * 0.02\n",
    "            )\n",
    "\n",
    "            flood = 1 if flood_score > 0.45 else 0\n",
    "            if np.random.random() < 0.03:\n",
    "                flood = 1 - flood\n",
    "\n",
    "            data.append([\n",
    "                rainfall, river_discharge, water_level, soil_moisture, temperature,\n",
    "                humidity, wind_speed, pressure, elevation, population_density,\n",
    "                drainage_efficiency, distance_to_coast, deforestation, flood\n",
    "            ])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=self.feature_columns + ['flood_occurred'])\n",
    "        print(f\"✅ Dataset ready: {df.shape}, Flood cases: {df['flood_occurred'].sum()}\")\n",
    "        return df\n",
    "\n",
    "    def train_model(self, df=None):\n",
    "        if df is None:\n",
    "            df = self.generate_physical_dataset()\n",
    "\n",
    "        X = df[self.feature_columns]\n",
    "        y = df['flood_occurred']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "        # Preprocessing + Model Pipeline\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', Pipeline([\n",
    "                ('power', PowerTransformer(method='yeo-johnson')),\n",
    "                ('scale', StandardScaler())\n",
    "            ]), self.feature_columns)\n",
    "        ])\n",
    "\n",
    "        base_model = HistGradientBoostingClassifier(max_iter=250, learning_rate=0.08, max_depth=6, random_state=42)\n",
    "        model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)\n",
    "        pipeline = Pipeline([\n",
    "            ('pre', preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        print(\"🔧 Training model on generalized data...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        print(f\"✅ Model trained! Accuracy: {acc:.3f}, ROC-AUC: {roc_auc:.3f}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Normal', 'Flood']))\n",
    "\n",
    "        self.model = pipeline\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict_flood_risk(self, **kwargs):\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"❌ Model not trained yet!\")\n",
    "\n",
    "        # ensure all parameters provided\n",
    "        for col in self.feature_columns:\n",
    "            if col not in kwargs:\n",
    "                raise ValueError(f\"Missing input: {col}\")\n",
    "\n",
    "        X = pd.DataFrame([[kwargs[col] for col in self.feature_columns]], columns=self.feature_columns)\n",
    "        prob = self.model.predict_proba(X)[0, 1]\n",
    "        pred = int(prob >= 0.5)\n",
    "\n",
    "        if prob >= 0.85:\n",
    "            risk = \"🚨 CRITICAL\"\n",
    "            rec = \"IMMEDIATE EVACUATION may be required!\"\n",
    "        elif prob >= 0.65:\n",
    "            risk = \"🔴 HIGH\"\n",
    "            rec = \"Prepare for potential evacuation.\"\n",
    "        elif prob >= 0.45:\n",
    "            risk = \"🟠 MEDIUM\"\n",
    "            rec = \"Monitor conditions closely.\"\n",
    "        elif prob >= 0.25:\n",
    "            risk = \"🟡 LOW\"\n",
    "            rec = \"Stay alert and monitor weather updates.\"\n",
    "        else:\n",
    "            risk = \"🟢 SAFE\"\n",
    "            rec = \"Conditions normal.\"\n",
    "\n",
    "        return {\n",
    "            \"prediction\": \"FLOOD WARNING\" if pred == 1 else \"NORMAL CONDITIONS\",\n",
    "            \"probability\": round(prob, 3),\n",
    "            \"risk_level\": risk,\n",
    "            \"recommendation\": rec,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee72e7bc-c27a-4f5a-9f87-eac776840aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset ready: (8000, 14), Flood cases: 787\n",
      "🔧 Training model on generalized data...\n",
      "✅ Model trained! Accuracy: 0.970, ROC-AUC: 0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.97      0.99      0.98      1803\n",
      "       Flood       0.93      0.76      0.83       197\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.95      0.87      0.91      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PhysicalFloodPredictor at 0x147ff3cf2d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = PhysicalFloodPredictor()\n",
    "predictor.train_model()  # Generates synthetic data + trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d29cfae-fd7e-497c-900a-8d6a7db725a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌍 Flood Prediction Output:\n",
      "prediction: NORMAL CONDITIONS\n",
      "probability: 0.492\n",
      "risk_level: 🟠 MEDIUM\n",
      "recommendation: Monitor conditions closely.\n",
      "timestamp: 2025-10-26 12:09:49\n"
     ]
    }
   ],
   "source": [
    "# Example sample input (typical monsoon conditions)\n",
    "sample_input = {\n",
    "    'rainfall_mm': 100.0,\n",
    "    'river_discharge_cumec': 400.0,\n",
    "    'water_level_m': 3.2,\n",
    "    'soil_moisture_percent': 70.0,\n",
    "    'temperature_c': 27.5,\n",
    "    'humidity_percent': 88.0,\n",
    "    'wind_speed_ms': 12.0,\n",
    "    'pressure_hpa': 995.0,\n",
    "    'elevation_m': 200.0,\n",
    "    'population_density': 1500.0,\n",
    "    'drainage_efficiency': 0.3,\n",
    "    'distance_to_coast_km': 100.0,\n",
    "    'deforestation_index': 0.45\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "output = predictor.predict_flood_risk(**sample_input)\n",
    "print(\"\\n🌍 Flood Prediction Output:\")\n",
    "for k, v in output.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c332914-7c27-46a6-8886-352ba9e987d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "===========================================================================\n",
      "ERROR: 'spektral' library not found. Please run: pip install spektral\n",
      "===========================================================================\n",
      "🌊 PHASE 4: SPATIO-TEMPORAL GNN (STGNN) FORECASTING SYSTEM - DTU TECH TRAILBLAZERS\n",
      "===========================================================================\n",
      "ℹ No model files found or error loading: 'GraphFloodPredictor' object has no attribute 'model_dir'. Please train a new model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GraphFloodPredictor' object has no attribute 'is_trained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 391\u001b[0m\n\u001b[0;32m    387\u001b[0m predictor \u001b[38;5;241m=\u001b[39m GraphFloodPredictor()\n\u001b[0;32m    389\u001b[0m predictor\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_trained\u001b[49m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- No saved STGNN model found, training new system... ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    393\u001b[0m     predictor\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GraphFloodPredictor' object has no attribute 'is_trained'"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 🌊 PHASE 4: SPATIO-TEMPORAL GRAPH NEURAL NETWORK (STGNN) SYSTEM\n",
    "# ================================================================\n",
    "#\n",
    "# This model analyzes an entire network of sensors as a graph,\n",
    "# learning how upstream events propagate downstream.\n",
    "#\n",
    "# REQUIRED: pip install tensorflow spektral joblib pandas numpy\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import warnings, os\n",
    "from collections import deque\n",
    "\n",
    "# --- AI & Graph Libraries ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Spektral is the GNN library\n",
    "try:\n",
    "    import spektral\n",
    "    from spektral.layers import GraphConv\n",
    "    from spektral.data import Graph, Dataset\n",
    "    from spektral.transforms import GCNConv\n",
    "except ImportError:\n",
    "    print(\"=\"*75)\n",
    "    print(\"ERROR: 'spektral' library not found. Please run: pip install spektral\")\n",
    "    print(\"=\"*75)\n",
    "    exit()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🌊 PHASE 4: SPATIO-TEMPORAL GNN (STGNN) FORECASTING SYSTEM - DTU TECH TRAILBLAZERS\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "\n",
    "class GraphFloodPredictor:\n",
    "    def _init_(self, model_dir=\"model_stgnn\", sequence_length=72):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.is_trained = False\n",
    "        self.sequence_length = sequence_length # Look at 72 hours\n",
    "        \n",
    "        # --- Node/Sensor Network Definition ---\n",
    "        self.sensor_nodes = {\n",
    "            0: \"Prayagraj (Upstream)\",\n",
    "            1: \"Varanasi (Midstream)\",\n",
    "            2: \"Patna (Downstream)\"\n",
    "        }\n",
    "        self.num_nodes = len(self.sensor_nodes)\n",
    "        \n",
    "        # Adjacency Matrix (Graph \"Map\"): Defines river flow\n",
    "        # 1 means a connection exists (including self-loops)\n",
    "        self.adjacency_matrix = np.array([\n",
    "            [1, 1, 0],  # Prayagraj -> Varanasi\n",
    "            [0, 1, 1],  # Varanasi -> Patna\n",
    "            [0, 0, 1]   # Patna (no outflow in our model)\n",
    "        ], dtype=float)\n",
    "        # Add reverse connections (for GNN message passing) & normalize\n",
    "        self.adjacency_matrix = GraphConv.preprocess(self.adjacency_matrix)\n",
    "\n",
    "        # --- Base Features (per-sensor) ---\n",
    "        self.base_feature_columns = [\n",
    "            'rainfall_mm', 'river_discharge_cumec', 'water_level_m',\n",
    "            'soil_moisture_percent', 'temperature_c', 'humidity_percent'\n",
    "        ]\n",
    "        self.n_features = len(self.base_feature_columns)\n",
    "        \n",
    "        # --- Persistence ---\n",
    "        self.model_dir = model_dir\n",
    "        self.model_path = os.path.join(model_dir, \"flood_model_stgnn.h5\")\n",
    "        self.scaler_path = os.path.join(model_dir, \"flood_scaler_stgnn.joblib\")\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "        self.db_path = \"flood_cache_graph.db\"\n",
    "        self._setup_database()\n",
    "\n",
    "    # -------------------- DATABASE (OFFLINE LOGGING) --------------------\n",
    "    def _setup_database(self):\n",
    "        # (Same as before, logs predictions)\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"\"\"CREATE TABLE IF NOT EXISTS flood_logs (\n",
    "                        timestamp TEXT, location TEXT, probability REAL,\n",
    "                        risk_level TEXT, recommendation TEXT)\"\"\")\n",
    "        conn.close()\n",
    "\n",
    "    def log_prediction(self, location, probability, risk_level, recommendation):\n",
    "        # (Same as before)\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"INSERT INTO flood_logs VALUES (?, ?, ?, ?, ?)\",\n",
    "                     (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                      location, probability, risk_level, recommendation))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    # -------------------- ADVANCED DATA GENERATION (GRAPH) --------------------\n",
    "    def generate_graph_dataset(self, n_hours=10000):\n",
    "        print(f\"🧬 Generating {n_hours} hours of synthetic graph data for {self.num_nodes} nodes...\")\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # --- Create Time-Varying Features for ALL nodes ---\n",
    "        # Shape: (n_hours, num_nodes, n_features)\n",
    "        X_data = np.zeros((n_hours, self.num_nodes, self.n_features))\n",
    "        \n",
    "        # Store rainfall separately to calculate flow\n",
    "        rain_data = np.zeros((n_hours, self.num_nodes))\n",
    "        \n",
    "        # --- Define Node Properties (e.g., lag time from upstream) ---\n",
    "        # Lag (in hours) for discharge from upstream node\n",
    "        flow_lag = { 1: 12, 2: 18 } # 12h: Pryj->Vns, 18h: Vns->Pat\n",
    "        \n",
    "        for n in range(self.num_nodes):\n",
    "            time = np.arange(n_hours)\n",
    "            temp = 25 + 10 * np.sin(2 * np.pi * time / (365 * 24)) + np.random.normal(0, 1.5, n_hours)\n",
    "            \n",
    "            rain = np.random.exponential(0.5, n_hours)\n",
    "            storm_indices = np.random.randint(0, n_hours, n_hours // 100)\n",
    "            rain[storm_indices] += np.random.uniform(20, 80, len(storm_indices))\n",
    "            rain = np.clip(rain, 0, 200)\n",
    "            rain_data[:, n] = rain\n",
    "            \n",
    "            humidity = np.clip(50 + rain * 0.5 + (temp - 25), 20, 100)\n",
    "            \n",
    "            soil = np.zeros(n_hours)\n",
    "            soil[0] = 30\n",
    "            for i in range(1, n_hours):\n",
    "                soil[i] = np.clip(soil[i-1] + rain[i]*0.5 - 1.0, 10, 100)\n",
    "                \n",
    "            # --- Store in main matrix ---\n",
    "            X_data[:, n, 0] = rain\n",
    "            X_data[:, n, 3] = soil\n",
    "            X_data[:, n, 4] = temp\n",
    "            X_data[:, n, 5] = humidity\n",
    "            \n",
    "        # --- Simulate Hydrological Flow (The \"Graph\" part) ---\n",
    "        print(\"Simulating downstream hydrological flow...\")\n",
    "        discharge_data = np.zeros((n_hours, self.num_nodes))\n",
    "        water_level_data = np.zeros((n_hours, self.num_nodes))\n",
    "\n",
    "        for n in range(self.num_nodes):\n",
    "            # Base discharge from local rainfall\n",
    "            local_discharge = pd.Series(rain_data[:, n]).rolling(48).sum() * 0.2 + 5\n",
    "            \n",
    "            # Add discharge from upstream node (with lag)\n",
    "            upstream_discharge = 0\n",
    "            if n == 1: # Varanasi\n",
    "                upstream_rain = pd.Series(rain_data[:, 0]).shift(flow_lag[n])\n",
    "                upstream_discharge = upstream_rain.rolling(48).sum() * 0.4\n",
    "            elif n == 2: # Patna\n",
    "                upstream_rain = pd.Series(rain_data[:, 1]).shift(flow_lag[n])\n",
    "                upstream_discharge = upstream_rain.rolling(48).sum() * 0.5\n",
    "            \n",
    "            total_discharge = local_discharge.fillna(5) + upstream_discharge.fillna(0)\n",
    "            discharge_data[:, n] = total_discharge\n",
    "            water_level_data[:, n] = 1.5 + total_discharge / 50 + np.random.normal(0, 0.1, n_hours)\n",
    "\n",
    "        X_data[:, :, 1] = discharge_data\n",
    "        X_data[:, :, 2] = water_level_data\n",
    "\n",
    "        # --- Define Flood Event (per-node) ---\n",
    "        # Shape: (n_hours, num_nodes)\n",
    "        water_level_risk = water_level_data / 8.0 # 8m = critical level\n",
    "        rain_risk = pd.DataFrame(rain_data).rolling(24).sum().values / 150\n",
    "        \n",
    "        flood_risk = (water_level_risk * 0.7) + (rain_risk * 0.3)\n",
    "        y_data = (flood_risk > 0.65).astype(int)\n",
    "        \n",
    "        # Clean NaNs from rolling/lag\n",
    "        X_data = X_data[100:]\n",
    "        y_data = y_data[100:]\n",
    "        \n",
    "        print(f\"✅ Data generated. Total flood events: {np.sum(y_data, axis=0)}\")\n",
    "        return X_data, y_data, self.adjacency_matrix\n",
    "\n",
    "    # --- Helper to create ST-GNN sequences ---\n",
    "    def create_sequences(self, X_data, y_data):\n",
    "        X, y = [], []\n",
    "        # X_data shape: (time, nodes, features)\n",
    "        # y_data shape: (time, nodes)\n",
    "        for i in range(len(X_data) - self.sequence_length):\n",
    "            # Input sequence: (seq_len, nodes, features)\n",
    "            X.append(X_data[i:(i + self.sequence_length)])\n",
    "            \n",
    "            # Output label: (nodes, 1) - risk at the end of the sequence\n",
    "            y.append(y_data[i + self.sequence_length - 1])\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # -------------------- MASTER TRAINING PIPELINE (STGNN) --------------------\n",
    "    def train_model(self, X_data=None, y_data=None, adj_matrix=None):\n",
    "        if X_data is None:\n",
    "            X_data, y_data, adj_matrix = self.generate_graph_dataset()\n",
    "        \n",
    "        # --- 1. Scale Features ---\n",
    "        # We scale features before creating sequences\n",
    "        # X_data shape: (n_hours, num_nodes, n_features)\n",
    "        print(\"Scaling features...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Fit the scaler on (n_hours * num_nodes, n_features)\n",
    "        X_data_reshaped = X_data.reshape(-1, self.n_features)\n",
    "        self.scaler.fit(X_data_reshaped)\n",
    "        \n",
    "        # Transform back to original shape\n",
    "        X_data_scaled = self.scaler.transform(X_data_reshaped).reshape(X_data.shape)\n",
    "        \n",
    "        # --- 2. Create Sequences ---\n",
    "        print(f\"🔄 Creating sequences of length {self.sequence_length}...\")\n",
    "        # X shape: (samples, seq_len, nodes, features)\n",
    "        # y shape: (samples, nodes)\n",
    "        X_seq, y_seq = self.create_sequences(X_data_scaled, y_data)\n",
    "        \n",
    "        # Reshape y to (samples, nodes, 1) for the model output\n",
    "        y_seq = y_seq.reshape(y_seq.shape[0], y_seq.shape[1], 1)\n",
    "        \n",
    "        # --- 3. Build the STGNN Model ---\n",
    "        print(\"🧠 Building Spatio-Temporal GNN (STGNN) model...\")\n",
    "        \n",
    "        # Input shapes\n",
    "        X_in = Input(shape=(self.sequence_length, self.num_nodes, self.n_features))\n",
    "        A_in = Input(shape=(self.num_nodes, self.num_nodes), sparse=True)\n",
    "        \n",
    "        # We process the graph at each time step\n",
    "        # Use TimeDistributed to apply the same GNN layer to each of the 72 timesteps\n",
    "        \n",
    "        # Spatial Layer 1 (GNN)\n",
    "        graph_conv_1 = TimeDistributed(GraphConv(32, activation='relu'))([X_in, A_in])\n",
    "        \n",
    "        # Spatial Layer 2 (GNN)\n",
    "        graph_conv_2 = TimeDistributed(GraphConv(16, activation='relu'))([graph_conv_1, A_in])\n",
    "        \n",
    "        # Temporal Layers (LSTM)\n",
    "        # We need to reshape for LSTM. The LSTM should see the output of the GNNs\n",
    "        # for each node as its features.\n",
    "        # (samples, seq_len, nodes, 16) -> (samples, nodes, seq_len, 16)\n",
    "        lstm_input = tf.transpose(graph_conv_2, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Apply an LSTM to each node's timeline\n",
    "        # TimeDistributed here applies the same LSTM to each of the 3 nodes\n",
    "        lstm_out = TimeDistributed(LSTM(32, activation='relu', dropout=0.2))(lstm_input)\n",
    "        \n",
    "        # Output Layer\n",
    "        # We need a risk probability (0-1) for each node\n",
    "        output = TimeDistributed(Dense(1, activation='sigmoid'))(lstm_out)\n",
    "        \n",
    "        # Build Model\n",
    "        self.model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "\n",
    "        # --- 4. Train Model ---\n",
    "        # Create a static Adjacency Matrix input for all samples\n",
    "        A_train = np.array([adj_matrix] * X_seq.shape[0])\n",
    "        \n",
    "        X_train, X_test, A_train_s, A_test_s, y_train, y_test = train_test_split(\n",
    "            X_seq, A_train, y_seq, test_size=0.2, shuffle=False\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        print(\"🔧 Training STGNN model...\")\n",
    "        self.model.fit(\n",
    "            [X_train, A_train_s], y_train,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            validation_data=([X_test, A_test_s], y_test),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"✅ STGNN Model Trained.\")\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    # -------------------- MODEL PERSISTENCE --------------------\n",
    "    def save_model(self):\n",
    "        if self.is_trained:\n",
    "            print(f\"💾 Saving model to {self.model_dir}...\")\n",
    "            self.model.save(self.model_path)\n",
    "            joblib.dump(self.scaler, self.scaler_path)\n",
    "            print(\"✅ Model and scaler saved.\")\n",
    "        else:\n",
    "            print(\"⚠ Cannot save, model is not trained.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading model from {self.model_dir}...\")\n",
    "            self.model = keras.models.load_model(self.model_path)\n",
    "            self.scaler = joblib.load(self.scaler_path)\n",
    "            self.is_trained = True\n",
    "            print(\"✅ Model and scaler loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ℹ No model files found or error loading: {e}. Please train a new model.\")\n",
    "\n",
    "    # -------------------- PREDICTION (GRAPH-BASED) --------------------\n",
    "    def predict(self, history_data_all_nodes=None):\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"❌ Model not trained yet! Please run train_model() or load_model().\")\n",
    "        \n",
    "        # history_data_all_nodes shape: (seq_len, num_nodes, num_features)\n",
    "        if (not isinstance(history_data_all_nodes, np.ndarray) or\n",
    "            history_data_all_nodes.shape != (self.sequence_length, self.num_nodes, self.n_features)):\n",
    "            raise ValueError(f\"❌ 'history_data_all_nodes' must be a NumPy array with shape \"\n",
    "                             f\"({self.sequence_length}, {self.num_nodes}, {self.n_features})\")\n",
    "\n",
    "        # --- 1. Scale the input data ---\n",
    "        X_scaled_reshaped = self.scaler.transform(history_data_all_nodes.reshape(-1, self.n_features))\n",
    "        X_scaled = X_scaled_reshaped.reshape(self.sequence_length, self.num_nodes, self.n_features)\n",
    "        \n",
    "        # --- 2. Add batch dimension ---\n",
    "        X_batch = np.expand_dims(X_scaled, axis=0) # (1, seq_len, nodes, features)\n",
    "        A_batch = np.expand_dims(self.adjacency_matrix, axis=0) # (1, nodes, nodes)\n",
    "        \n",
    "        # --- 3. Predict ---\n",
    "        # Output shape: (1, num_nodes, 1)\n",
    "        probs = self.model.predict([X_batch, A_batch], verbose=0)[0]\n",
    "        \n",
    "        # --- 4. Format Output ---\n",
    "        results = {}\n",
    "        for n in range(self.num_nodes):\n",
    "            prob = probs[n, 0]\n",
    "            location_name = self.sensor_nodes[n]\n",
    "            \n",
    "            pred = \"FLOOD WARNING\" if prob > 0.5 else \"NORMAL CONDITIONS\"\n",
    "            if prob >= 0.85: risk, rec = \"🚨 CRITICAL\", \"IMMEDIATE EVACUATION may be required!\"\n",
    "            elif prob >= 0.65: risk, rec = \"🔴 HIGH\", \"Prepare for evacuation.\"\n",
    "            elif prob >= 0.45: risk, rec = \"🟠 MEDIUM\", \"Monitor closely.\"\n",
    "            elif prob >= 0.25: risk, rec = \"🟡 LOW\", \"Stay alert.\"\n",
    "            else: risk, rec = \"🟢 SAFE\", \"Conditions normal.\"\n",
    "            \n",
    "            self.log_prediction(location_name, float(prob), risk, rec)\n",
    "            \n",
    "            results[location_name] = {\n",
    "                \"prediction\": pred,\n",
    "                \"probability\": round(float(prob), 3),\n",
    "                \"risk_level\": risk,\n",
    "                \"recommendation\": rec\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    # -------------------- REAL-TIME SIMULATION --------------------\n",
    "    def simulate_real_time_feed(self, sim_hours=50):\n",
    "        print(\"\\n\\n--- 🔴 STARTING REAL-TIME GRAPH (STGNN) SIMULATION ---\")\n",
    "        X_data, _, _ = self.generate_graph_dataset(n_hours=self.sequence_length + sim_hours)\n",
    "        \n",
    "        # Use deque for an efficient rolling window of graph snapshots\n",
    "        # Each item in deque is a snapshot of shape (num_nodes, num_features)\n",
    "        history = deque(maxlen=self.sequence_length)\n",
    "        \n",
    "        print(f\"Bootstrapping model with first {self.sequence_length} hours of data...\")\n",
    "        for i in range(self.sequence_length):\n",
    "            history.append(X_data[i])\n",
    "        \n",
    "        print(\"⚡ Simulation LIVE. Predicting graph state hour-by-hour...\\n\")\n",
    "        \n",
    "        for i in range(self.sequence_length, self.sequence_length + sim_hours):\n",
    "            # Get the new data (snapshot for all nodes)\n",
    "            new_graph_snapshot = X_data[i] # Shape: (num_nodes, num_features)\n",
    "            history.append(new_graph_snapshot) \n",
    "            \n",
    "            # Make prediction using the full 72-hour history of the graph\n",
    "            # Convert deque to numpy array: (seq_len, num_nodes, num_features)\n",
    "            current_history_array = np.array(history)\n",
    "            \n",
    "            # Get predictions for ALL nodes\n",
    "            results = self.predict(history_data_all_nodes=current_history_array)\n",
    "            \n",
    "            print(f\"--- HOUR {i} ---\")\n",
    "            for node_name, result in results.items():\n",
    "                print(f\"  📍 {node_name:<25} -> {result['risk_level']:<10} (Prob: {result['probability']})\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# -------------------- RUN DEMO --------------------\n",
    "# -------------------- RUN DEMO --------------------\n",
    "if __name__ == '__main__':\n",
    "    predictor = GraphFloodPredictor()\n",
    "\n",
    "    predictor.load_model()\n",
    "\n",
    "    if not predictor.is_trained:\n",
    "        print(\"\\n--- No saved STGNN model found, training new system... ---\")\n",
    "        predictor.train_model()\n",
    "        predictor.save_model()\n",
    "        print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "    predictor.simulate_real_time_feed(sim_hours=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1934d6-da5d-4e79-a902-b3cca1fd418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spektral\n",
      "  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (1.3.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (5.2.1)\n",
      "Collecting networkx (from spektral)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (1.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (4.66.2)\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spektral) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow>=2.2.0->spektral) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->spektral) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->spektral) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->spektral) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->spektral) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->spektral) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->spektral) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->spektral) (2023.11.17)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->spektral) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->spektral) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\yashv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.2.0->spektral) (3.2.2)\n",
      "Downloading spektral-1.3.1-py3-none-any.whl (140 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.0 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: networkx, spektral\n",
      "Successfully installed networkx-3.5 spektral-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ded8de5-d2ed-4fe6-a4fd-f03c6fec9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ℹ No model files found or error loading: Model or scaler file not found.. Please train a new model.\n",
      "\n",
      "--- No saved STGNN model found or couldn't load it; training new system... ---\n",
      "🧬 Generating 10000 hours of synthetic graph data for 3 nodes...\n",
      "Simulating downstream hydrological flow...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 353\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predictor\u001b[38;5;241m.\u001b[39mis_trained:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- No saved STGNN model found or couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load it; training new system... ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 353\u001b[0m     \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shorten epochs for demo; raise for real training\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     predictor\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 202\u001b[0m, in \u001b[0;36mGraphFloodPredictor.train_model\u001b[1;34m(self, X_data, y_data, adj_matrix, epochs, batch_size)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, adj_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m y_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m adj_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m         X_data, y_data, adj_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaling features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 140\u001b[0m, in \u001b[0;36mGraphFloodPredictor.generate_graph_dataset\u001b[1;34m(self, n_hours)\u001b[0m\n\u001b[0;32m    137\u001b[0m     upstream_rain \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(rain_data[:, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mshift(flow_lag[n])\n\u001b[0;32m    138\u001b[0m     upstream_discharge \u001b[38;5;241m=\u001b[39m upstream_rain\u001b[38;5;241m.\u001b[39mrolling(\u001b[38;5;241m48\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m--> 140\u001b[0m total_discharge \u001b[38;5;241m=\u001b[39m local_discharge\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[43mupstream_discharge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    141\u001b[0m discharge_data[:, n] \u001b[38;5;241m=\u001b[39m total_discharge\n\u001b[0;32m    142\u001b[0m water_level_data[:, n] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m+\u001b[39m total_discharge \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, n_hours)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "# Complete fixed STGNN-style flood predictor (no spektral dependency)\n",
    "# - Fixes: __init__, __name__ main block, model save/load, EarlyStopping import\n",
    "# - Spatial processing done with adjacency-matrix aggregation + Dense layers (GNN-like)\n",
    "# - Requires: tensorflow, scikit-learn, joblib, pandas, numpy\n",
    "#\n",
    "# pip install tensorflow scikit-learn joblib pandas numpy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------- GraphFloodPredictor Class --------------------\n",
    "class GraphFloodPredictor:\n",
    "    def __init__(self, model_dir=\"model_stgnn\", sequence_length=72):\n",
    "        # Basic attributes\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.is_trained = False\n",
    "        self.sequence_length = sequence_length  # lookback hours\n",
    "        \n",
    "        # Node/sensor network\n",
    "        self.sensor_nodes = {\n",
    "            0: \"Prayagraj (Upstream)\",\n",
    "            1: \"Varanasi (Midstream)\",\n",
    "            2: \"Patna (Downstream)\"\n",
    "        }\n",
    "        self.num_nodes = len(self.sensor_nodes)\n",
    "        \n",
    "        # Adjacency matrix (simple river-flow map)\n",
    "        A = np.array([\n",
    "            [1.0, 1.0, 0.0],  # Prayagraj -> Varanasi\n",
    "            [0.0, 1.0, 1.0],  # Varanasi -> Patna\n",
    "            [0.0, 0.0, 1.0]\n",
    "        ], dtype=float)\n",
    "        # Add self-loops and symmetrize / normalize (symmetric normalization)\n",
    "        A = A + np.eye(self.num_nodes)\n",
    "        deg = np.sum(A, axis=1)\n",
    "        deg_inv_sqrt = np.diag(1.0 / np.sqrt(deg))\n",
    "        self.adjacency_matrix = deg_inv_sqrt @ A @ deg_inv_sqrt  # normalized adjacency\n",
    "\n",
    "        # Feature columns per node\n",
    "        self.base_feature_columns = [\n",
    "            'rainfall_mm', 'river_discharge_cumec', 'water_level_m',\n",
    "            'soil_moisture_percent', 'temperature_c', 'humidity_percent'\n",
    "        ]\n",
    "        self.n_features = len(self.base_feature_columns)\n",
    "        \n",
    "        # Persistence paths\n",
    "        self.model_dir = model_dir\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        self.model_path = os.path.join(self.model_dir, \"flood_model_stgnn.keras\")\n",
    "        self.scaler_path = os.path.join(self.model_dir, \"flood_scaler_stgnn.joblib\")\n",
    "        \n",
    "        # DB for logs\n",
    "        self.db_path = \"flood_cache_graph.db\"\n",
    "        self._setup_database()\n",
    "\n",
    "    # -------------------- DB --------------------\n",
    "    def _setup_database(self):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"\"\"CREATE TABLE IF NOT EXISTS flood_logs (\n",
    "                        timestamp TEXT, location TEXT, probability REAL,\n",
    "                        risk_level TEXT, recommendation TEXT)\"\"\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def log_prediction(self, location, probability, risk_level, recommendation):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"INSERT INTO flood_logs VALUES (?, ?, ?, ?, ?)\",\n",
    "                     (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                      location, probability, risk_level, recommendation))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    # -------------------- Synthetic Graph Data --------------------\n",
    "    def generate_graph_dataset(self, n_hours=10000):\n",
    "        print(f\"🧬 Generating {n_hours} hours of synthetic graph data for {self.num_nodes} nodes...\")\n",
    "        np.random.seed(42)\n",
    "\n",
    "        X_data = np.zeros((n_hours, self.num_nodes, self.n_features))\n",
    "        rain_data = np.zeros((n_hours, self.num_nodes))\n",
    "\n",
    "        # Lag times for flow\n",
    "        flow_lag = {1: 12, 2: 18}\n",
    "\n",
    "        for n in range(self.num_nodes):\n",
    "            time = np.arange(n_hours)\n",
    "            temp = 25 + 10 * np.sin(2 * np.pi * time / (365 * 24)) + np.random.normal(0, 1.5, n_hours)\n",
    "\n",
    "            rain = np.random.exponential(0.5, n_hours)\n",
    "            storm_indices = np.random.randint(0, n_hours, n_hours // 100)\n",
    "            rain[storm_indices] += np.random.uniform(20, 80, len(storm_indices))\n",
    "            rain = np.clip(rain, 0, 200)\n",
    "            rain_data[:, n] = rain\n",
    "\n",
    "            humidity = np.clip(50 + rain * 0.5 + (temp - 25), 20, 100)\n",
    "\n",
    "            soil = np.zeros(n_hours)\n",
    "            soil[0] = 30\n",
    "            for i in range(1, n_hours):\n",
    "                soil[i] = np.clip(soil[i - 1] + rain[i] * 0.5 - 1.0, 10, 100)\n",
    "\n",
    "            X_data[:, n, 0] = rain\n",
    "            X_data[:, n, 3] = soil\n",
    "            X_data[:, n, 4] = temp\n",
    "            X_data[:, n, 5] = humidity\n",
    "\n",
    "        # Simulate discharge and water level using upstream contributions\n",
    "        print(\"Simulating downstream hydrological flow...\")\n",
    "        discharge_data = np.zeros((n_hours, self.num_nodes))\n",
    "        water_level_data = np.zeros((n_hours, self.num_nodes))\n",
    "\n",
    "        for n in range(self.num_nodes):\n",
    "            local_discharge = pd.Series(rain_data[:, n]).rolling(48).sum() * 0.2 + 5\n",
    "\n",
    "            upstream_discharge = 0\n",
    "            if n == 1:  # Varanasi gets from Prayagraj\n",
    "                upstream_rain = pd.Series(rain_data[:, 0]).shift(flow_lag[n])\n",
    "                upstream_discharge = upstream_rain.rolling(48).sum() * 0.4\n",
    "            elif n == 2:  # Patna gets from Varanasi\n",
    "                upstream_rain = pd.Series(rain_data[:, 1]).shift(flow_lag[n])\n",
    "                upstream_discharge = upstream_rain.rolling(48).sum() * 0.5\n",
    "\n",
    "            total_discharge = local_discharge.fillna(5) + upstream_discharge.fillna(0)\n",
    "            discharge_data[:, n] = total_discharge\n",
    "            water_level_data[:, n] = 1.5 + total_discharge / 50 + np.random.normal(0, 0.1, n_hours)\n",
    "\n",
    "        X_data[:, :, 1] = discharge_data\n",
    "        X_data[:, :, 2] = water_level_data\n",
    "\n",
    "        water_level_risk = water_level_data / 8.0  # 8m critical\n",
    "        rain_risk = pd.DataFrame(rain_data).rolling(24).sum().values / 150.0\n",
    "\n",
    "        flood_risk = (water_level_risk * 0.7) + (rain_risk * 0.3)\n",
    "        y_data = (flood_risk > 0.65).astype(int)\n",
    "\n",
    "        # Remove early NaN-ish rows\n",
    "        X_data = X_data[100:]\n",
    "        y_data = y_data[100:]\n",
    "\n",
    "        print(f\"✅ Data generated. Total flood events per node: {np.sum(y_data, axis=0)}\")\n",
    "        return X_data, y_data, self.adjacency_matrix\n",
    "\n",
    "    # -------------------- Sequence creator --------------------\n",
    "    def create_sequences(self, X_data, y_data):\n",
    "        X, y = [], []\n",
    "        for i in range(len(X_data) - self.sequence_length):\n",
    "            X.append(X_data[i:(i + self.sequence_length)])\n",
    "            y.append(y_data[i + self.sequence_length - 1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # -------------------- Build & Train Model --------------------\n",
    "    def build_model(self):\n",
    "        # Inputs\n",
    "        X_in = Input(shape=(self.sequence_length, self.num_nodes, self.n_features), name=\"X_in\")\n",
    "        A_in = Input(shape=(self.num_nodes, self.num_nodes), name=\"A_in\")\n",
    "\n",
    "        # Spatial aggregation per time-step: result shape -> (batch, seq, nodes, features)\n",
    "        def spatial_aggregate(inputs):\n",
    "            A, X = inputs  # A: (batch, nodes, nodes), X: (batch, seq, nodes, features)\n",
    "            # einsum sums over j: out[b,t,i,f] = sum_j A[b,i,j] * X[b,t,j,f]\n",
    "            return tf.einsum('bij,btjf->btif', A, X)\n",
    "\n",
    "        agg = Lambda(spatial_aggregate, name=\"graph_aggregate\")([A_in, X_in])\n",
    "\n",
    "        # After aggregation, apply Dense feature transforms in time-distributed fashion (like GraphConv)\n",
    "        td1 = TimeDistributed(Dense(32, activation='relu'), name=\"spatial_dense_1\")(agg)\n",
    "        td2 = TimeDistributed(Dense(16, activation='relu'), name=\"spatial_dense_2\")(td1)\n",
    "\n",
    "        # Temporal processing: transpose to (batch, nodes, seq, features) so each node has a sequence\n",
    "        lstm_input = Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1, 3]), name=\"transpose_for_lstm\")(td2)\n",
    "\n",
    "        # Apply same LSTM for each node (TimeDistributed wraps LSTM which sees (seq, features) per node)\n",
    "        lstm_out = TimeDistributed(LSTM(32, activation='tanh', dropout=0.2, return_sequences=False),\n",
    "                                   name=\"per_node_lstm\")(lstm_input)\n",
    "\n",
    "        # Output: per-node probability\n",
    "        outputs = TimeDistributed(Dense(1, activation='sigmoid'), name=\"node_output\")(lstm_out)\n",
    "\n",
    "        model = Model(inputs=[X_in, A_in], outputs=outputs, name=\"STGNN_like\")\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, X_data=None, y_data=None, adj_matrix=None, epochs=30, batch_size=32):\n",
    "        if X_data is None or y_data is None or adj_matrix is None:\n",
    "            X_data, y_data, adj_matrix = self.generate_graph_dataset()\n",
    "\n",
    "        # Scale features\n",
    "        print(\"Scaling features...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        X_data_reshaped = X_data.reshape(-1, self.n_features)\n",
    "        self.scaler.fit(X_data_reshaped)\n",
    "        X_data_scaled = self.scaler.transform(X_data_reshaped).reshape(X_data.shape)\n",
    "\n",
    "        print(f\"🔄 Creating sequences of length {self.sequence_length}...\")\n",
    "        X_seq, y_seq = self.create_sequences(X_data_scaled, y_data)\n",
    "        y_seq = y_seq.reshape(y_seq.shape[0], y_seq.shape[1], 1)  # (samples, nodes, 1)\n",
    "\n",
    "        print(\"🧠 Building model...\")\n",
    "        self.model = self.build_model()\n",
    "        self.model.summary()\n",
    "\n",
    "        # Prepare adjacency batch (same adj for all samples)\n",
    "        A_batch = np.array([adj_matrix] * X_seq.shape[0])\n",
    "\n",
    "        # Split (note: shuffle=False to respect time ordering)\n",
    "        X_train, X_test, A_train_s, A_test_s, y_train, y_test = train_test_split(\n",
    "            X_seq, A_batch, y_seq, test_size=0.2, shuffle=False\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        print(\"🔧 Training STGNN-like model...\")\n",
    "        self.model.fit(\n",
    "            [X_train, A_train_s], y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=([X_test, A_test_s], y_test),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"✅ STGNN-like Model Trained.\")\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    # -------------------- Persistence --------------------\n",
    "    def save_model(self):\n",
    "        if self.is_trained and self.model is not None:\n",
    "            os.makedirs(self.model_dir, exist_ok=True)\n",
    "            print(f\"💾 Saving model to {self.model_path} ...\")\n",
    "            try:\n",
    "                # Keras native save\n",
    "                self.model.save(self.model_path)\n",
    "                # Save scaler\n",
    "                joblib.dump(self.scaler, self.scaler_path)\n",
    "                print(\"✅ Model and scaler saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Error saving model: {e}\")\n",
    "        else:\n",
    "            print(\"⚠ Cannot save, model is not trained or missing.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):\n",
    "                print(f\"Loading model from {self.model_path} ...\")\n",
    "                self.model = keras.models.load_model(self.model_path, compile=True)\n",
    "                self.scaler = joblib.load(self.scaler_path)\n",
    "                self.is_trained = True\n",
    "                print(\"✅ Model and scaler loaded.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Model or scaler file not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ℹ No model files found or error loading: {e}. Please train a new model.\")\n",
    "            self.model = None\n",
    "            self.scaler = None\n",
    "            self.is_trained = False\n",
    "\n",
    "    # -------------------- Prediction --------------------\n",
    "    def predict(self, history_data_all_nodes=None):\n",
    "        if not self.is_trained or self.model is None:\n",
    "            raise ValueError(\"❌ Model not trained yet! Please run train_model() or load_model().\")\n",
    "\n",
    "        if (not isinstance(history_data_all_nodes, np.ndarray) or\n",
    "            history_data_all_nodes.shape != (self.sequence_length, self.num_nodes, self.n_features)):\n",
    "            raise ValueError(\n",
    "                f\"❌ 'history_data_all_nodes' must be a NumPy array with shape \"\n",
    "                f\"({self.sequence_length}, {self.num_nodes}, {self.n_features})\"\n",
    "            )\n",
    "\n",
    "        # Scale the input\n",
    "        X_scaled_reshaped = self.scaler.transform(history_data_all_nodes.reshape(-1, self.n_features))\n",
    "        X_scaled = X_scaled_reshaped.reshape(self.sequence_length, self.num_nodes, self.n_features)\n",
    "\n",
    "        # Batch dimension\n",
    "        X_batch = np.expand_dims(X_scaled, axis=0)  # (1, seq_len, nodes, features)\n",
    "        A_batch = np.expand_dims(self.adjacency_matrix, axis=0)  # (1, nodes, nodes)\n",
    "\n",
    "        probs = self.model.predict([X_batch, A_batch], verbose=0)[0]  # (nodes, 1)\n",
    "        results = {}\n",
    "        for n in range(self.num_nodes):\n",
    "            prob = float(probs[n, 0])\n",
    "            location_name = self.sensor_nodes[n]\n",
    "\n",
    "            pred = \"FLOOD WARNING\" if prob > 0.5 else \"NORMAL CONDITIONS\"\n",
    "            if prob >= 0.85:\n",
    "                risk, rec = \"🚨 CRITICAL\", \"IMMEDIATE EVACUATION may be required!\"\n",
    "            elif prob >= 0.65:\n",
    "                risk, rec = \"🔴 HIGH\", \"Prepare for evacuation.\"\n",
    "            elif prob >= 0.45:\n",
    "                risk, rec = \"🟠 MEDIUM\", \"Monitor closely.\"\n",
    "            elif prob >= 0.25:\n",
    "                risk, rec = \"🟡 LOW\", \"Stay alert.\"\n",
    "            else:\n",
    "                risk, rec = \"🟢 SAFE\", \"Conditions normal.\"\n",
    "\n",
    "            self.log_prediction(location_name, prob, risk, rec)\n",
    "            results[location_name] = {\n",
    "                \"prediction\": pred,\n",
    "                \"probability\": round(prob, 3),\n",
    "                \"risk_level\": risk,\n",
    "                \"recommendation\": rec\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    # -------------------- Real-time simulation --------------------\n",
    "    def simulate_real_time_feed(self, sim_hours=50):\n",
    "        print(\"\\n\\n--- 🔴 STARTING REAL-TIME GRAPH (STGNN-like) SIMULATION ---\")\n",
    "        X_data, _, _ = self.generate_graph_dataset(n_hours=self.sequence_length + sim_hours)\n",
    "\n",
    "        history = deque(maxlen=self.sequence_length)\n",
    "        print(f\"Bootstrapping model with first {self.sequence_length} hours of data...\")\n",
    "        for i in range(self.sequence_length):\n",
    "            history.append(X_data[i])\n",
    "\n",
    "        print(\"⚡ Simulation LIVE. Predicting hour-by-hour...\\n\")\n",
    "        for i in range(self.sequence_length, self.sequence_length + sim_hours):\n",
    "            new_graph_snapshot = X_data[i]  # (nodes, features)\n",
    "            history.append(new_graph_snapshot)\n",
    "            current_history_array = np.array(history)  # (seq_len, nodes, features)\n",
    "            results = self.predict(history_data_all_nodes=current_history_array)\n",
    "\n",
    "            print(f\"--- HOUR {i} ---\")\n",
    "            for node_name, result in results.items():\n",
    "                print(f\"  📍 {node_name:<25} -> {result['risk_level']:<10} (Prob: {result['probability']})\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# -------------------- RUN DEMO --------------------\n",
    "if __name__ == '__main__':\n",
    "    predictor = GraphFloodPredictor()\n",
    "\n",
    "    # Try to load a pre-trained model\n",
    "    predictor.load_model()\n",
    "\n",
    "    if not predictor.is_trained:\n",
    "        print(\"\\n--- No saved STGNN model found or couldn't load it; training new system... ---\")\n",
    "        predictor.train_model(epochs=15)  # shorten epochs for demo; raise for real training\n",
    "        predictor.save_model()\n",
    "        print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "    # Run a short real-time simulation (adjust sim_hours as needed)\n",
    "    predictor.simulate_real_time_feed(sim_hours=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee80470-f35c-4965-8fdf-95575c60d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ No saved model found. Will train a new one.\n",
      "\n",
      "--- Training new model... ---\n",
      "🌧 Generating synthetic rainfall and water level data...\n",
      "WARNING:tensorflow:From C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "🚀 Training model...\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:From C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yashv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "154/154 [==============================] - 31s 95ms/step - loss: 0.0999 - accuracy: 0.9903\n",
      "Epoch 2/8\n",
      "154/154 [==============================] - 16s 101ms/step - loss: 0.0309 - accuracy: 0.9951\n",
      "Epoch 3/8\n",
      "154/154 [==============================] - 9s 57ms/step - loss: 0.0309 - accuracy: 0.9951\n",
      "Epoch 4/8\n",
      "154/154 [==============================] - 5s 34ms/step - loss: 0.0308 - accuracy: 0.9951\n",
      "Epoch 5/8\n",
      "154/154 [==============================] - 5s 30ms/step - loss: 0.0309 - accuracy: 0.9951\n",
      "Epoch 6/8\n",
      "154/154 [==============================] - 5s 30ms/step - loss: 0.0309 - accuracy: 0.9951\n",
      "✅ Training complete!\n",
      "📦 Model and scaler saved!\n",
      "\n",
      "📡 Simulating 30 hours of real-time forecasting...\n",
      "🌊 Flood alerts: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "✅ Real-time simulation done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class GraphFloodPredictor:\n",
    "    def __init__(self, model_dir=\"model_simple\", sequence_length=72):\n",
    "        self.model_dir = model_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_trained = False\n",
    "\n",
    "    def generate_dataset(self, n_hours=5000):\n",
    "        print(\"🌧 Generating synthetic rainfall and water level data...\")\n",
    "        rng = np.random.default_rng(42)\n",
    "        rainfall = rng.uniform(0, 30, n_hours)\n",
    "        discharge = rainfall * 2 + rng.normal(0, 1, n_hours)\n",
    "        water_level = 1.5 + discharge * 0.05 + rng.normal(0, 0.2, n_hours)\n",
    "\n",
    "        data = np.stack([rainfall, discharge, water_level], axis=1)\n",
    "        df = pd.DataFrame(data, columns=[\"rainfall\", \"discharge\", \"water_level\"])\n",
    "        flood_flag = (df[\"water_level\"] > (df[\"water_level\"].mean() + 2 * df[\"water_level\"].std())).astype(int)\n",
    "\n",
    "        return df.values, flood_flag.values\n",
    "\n",
    "    def create_sequence_data(self, data, labels):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:i+self.sequence_length])\n",
    "            y.append(labels[i+self.sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            LSTM(32, return_sequences=True, input_shape=(self.sequence_length, 3)),\n",
    "            LSTM(16),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, epochs=8, batch_size=32):\n",
    "        data, labels = self.generate_dataset()\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "\n",
    "        X, y = self.create_sequence_data(scaled_data, labels)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        es = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "        print(\"🚀 Training model...\")\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[es])\n",
    "        self.is_trained = True\n",
    "        print(\"✅ Training complete!\")\n",
    "\n",
    "    def save_model(self):\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        self.model.save(os.path.join(self.model_dir, \"simple_model.keras\"))\n",
    "        np.save(os.path.join(self.model_dir, \"scaler.npy\"), self.scaler.mean_)\n",
    "        np.save(os.path.join(self.model_dir, \"scale.npy\"), self.scaler.scale_)\n",
    "        print(\"📦 Model and scaler saved!\")\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = load_model(os.path.join(self.model_dir, \"simple_model.keras\"))\n",
    "            self.scaler.mean_ = np.load(os.path.join(self.model_dir, \"scaler.npy\"))\n",
    "            self.scaler.scale_ = np.load(os.path.join(self.model_dir, \"scale.npy\"))\n",
    "            self.is_trained = True\n",
    "            print(\"✅ Model loaded successfully!\")\n",
    "        except:\n",
    "            print(\"ℹ No saved model found. Will train a new one.\")\n",
    "            self.is_trained = False\n",
    "\n",
    "    def predict_realtime(self, hours=50):\n",
    "        print(f\"\\n📡 Simulating {hours} hours of real-time forecasting...\")\n",
    "        rng = np.random.default_rng()\n",
    "        window = rng.uniform(0,1,(self.sequence_length,3))\n",
    "        flood_alert = []\n",
    "\n",
    "        for h in range(hours):\n",
    "            sample = window.reshape(1,self.sequence_length,3)\n",
    "            p = self.model.predict(sample, verbose=0)[0][0]\n",
    "            flood_alert.append(int(p>0.5))\n",
    "            new_data = rng.uniform(0,1,3)\n",
    "            window = np.vstack([window[1:], new_data])\n",
    "\n",
    "        print(\"🌊 Flood alerts:\", flood_alert[-10:])\n",
    "        print(\"✅ Real-time simulation done!\")\n",
    "\n",
    "# -------------------- RUN SCRIPT --------------------\n",
    "predictor = GraphFloodPredictor()\n",
    "predictor.load_model()\n",
    "\n",
    "if not predictor.is_trained:\n",
    "    print(\"\\n--- Training new model... ---\")\n",
    "    predictor.train_model(epochs=8)\n",
    "    predictor.save_model()\n",
    "\n",
    "predictor.predict_realtime(hours=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da35233-107c-4a9f-afd3-c43dbe5d8a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
